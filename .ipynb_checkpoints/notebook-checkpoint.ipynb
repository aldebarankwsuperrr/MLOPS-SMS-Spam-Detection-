{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5848dab",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, Tuner\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed455d",
   "metadata": {},
   "source": [
    "# Set Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1965db7f",
   "metadata": {},
   "source": [
    "Mendifinisikan variable-variable yang nantinya akan digunakan seperti nama pipeline dan nama skema pipeline. Selain itu pada bagian ini juga didefinisikan alamat directory dari akar pipeline, metadata, dataset, dan serving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"fahrul_firmansyah-pipeline\"\n",
    "SCHEMA_PIPELINE_NAME = \"fahrul_firmansyah-schema\"\n",
    "\n",
    "PIPELINE_ROOT = PIPELINE_NAME\n",
    "\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168dd3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0d128",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bfd759",
   "metadata": {},
   "source": [
    "Pada bagian ini, dataset yang berupa CSV file akan dimuat menggunakan komponen CsvExampleGen. Setelah dimuat dataset akan dibagi kedalam train dan eval dengan perbandingan 8:2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_context = InteractiveContext(pipeline_root = PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abfe8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = example_gen_pb2.Output(\n",
    "    split_config = example_gen_pb2.SplitConfig(splits=[\n",
    "        example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),\n",
    "        example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2)\n",
    "    ])\n",
    ")\n",
    "example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66c58b",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4f25a",
   "metadata": {},
   "source": [
    "Pada bagian data akan divalidasi menggunakan 3 tahapan yaitu:\n",
    "- Pembuatan summary statistics.\n",
    "- Pembuatan dat schema.\n",
    "- Pemeriksaan Anomali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f241cd",
   "metadata": {},
   "source": [
    "## Statistic Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_gen = StatisticsGen(\n",
    "    examples=example_gen.outputs[\"examples\"]\n",
    ")\n",
    " \n",
    " \n",
    "interactive_context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2249fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_context.show(statistics_gen.outputs[\"statistics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8e8a5",
   "metadata": {},
   "source": [
    "## Data Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edea1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_gen = SchemaGen(statistics=statistics_gen.outputs[\"statistics\"])\n",
    "interactive_context.run(schema_gen)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d5111",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interactive_context.show(schema_gen.outputs[\"schema\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e858528",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema']\n",
    ")\n",
    "interactive_context.run(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc36c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81d05a",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122691f",
   "metadata": {},
   "source": [
    "Pada bagian ini data yang sudah melewati tahap validasi akan diproses agar nantinya siap digunakan dalam pelatihan model. Semua proses transformasi akan disimpan dalam file berbeda dengan nama \"sms_transform.py\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95690fd",
   "metadata": {},
   "source": [
    "## Create Transform Module File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc19850",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_MODULE_FILE = \"sms_transform.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25627fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRANSFORM_MODULE_FILE}\n",
    "import tensorflow as tf\n",
    "LABEL_KEY = \"label\"\n",
    "FEATURE_KEY = \"sms\"\n",
    "def transformed_name(key):\n",
    "    \"\"\"Renaming transformed features\"\"\"\n",
    "    return key + \"_xf\"\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"\n",
    "    Preprocess input features into transformed features\n",
    "    \n",
    "    Args:\n",
    "        inputs: map from feature keys to raw features.\n",
    "    \n",
    "    Return:\n",
    "        outputs: map from feature keys to transformed features.    \n",
    "    \"\"\"\n",
    "    \n",
    "    outputs = {}\n",
    "    \n",
    "    outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])\n",
    "    \n",
    "    outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676ee93",
   "metadata": {},
   "source": [
    "# Transform Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2acd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform  = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema= schema_gen.outputs['schema'],\n",
    "    module_file=os.path.abspath(TRANSFORM_MODULE_FILE)\n",
    ")\n",
    "interactive_context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deccb5a2",
   "metadata": {},
   "source": [
    "# Training Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933afc2",
   "metadata": {},
   "source": [
    "Setelah data siap untuk dilatih, proses selanjutnya adalah melatih model. Seperti pada bagian data preprocessing, kode yang digunakan untuk membangun dan melatih machine learning model akan disimpan pada sebuah file yang berbeda dengan nama \"sms_trainer.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_MODULE_FILE = \"sms_trainer.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc36d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINER_MODULE_FILE}\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft \n",
    "from tensorflow.keras import layers\n",
    "import os  \n",
    "import tensorflow_hub as hub\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    " \n",
    "LABEL_KEY = \"label\"\n",
    "FEATURE_KEY = \"sms\"\n",
    " \n",
    "def transformed_name(key):\n",
    "    \"\"\"Renaming transformed features\"\"\"\n",
    "    return key + \"_xf\"\n",
    " \n",
    "def gzip_reader_fn(filenames):\n",
    "    \"\"\"Loads compressed data\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
    " \n",
    " \n",
    "def input_fn(file_pattern, \n",
    "             tf_transform_output,\n",
    "             num_epochs,\n",
    "             batch_size=64)->tf.data.Dataset:\n",
    "    \"\"\"Get post_tranform feature & create batches of data\"\"\"\n",
    "    \n",
    "    # Get post_transform feature spec\n",
    "    transform_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy())\n",
    "    \n",
    "    # create batches of data\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transform_feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        num_epochs=num_epochs,\n",
    "        label_key = transformed_name(LABEL_KEY))\n",
    "    return dataset\n",
    " \n",
    "# os.environ['TFHUB_CACHE_DIR'] = '/hub_chace'\n",
    "# embed = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    " \n",
    "# Vocabulary size and number of words in a sequence.\n",
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 100\n",
    " \n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH)\n",
    " \n",
    " \n",
    "embedding_dim=16\n",
    "def model_builder():\n",
    "    \"\"\"Build machine learning model\"\"\"\n",
    "    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n",
    "    reshaped_narrative = tf.reshape(inputs, [-1])\n",
    "    x = vectorize_layer(reshaped_narrative)\n",
    "    x = layers.Embedding(VOCAB_SIZE, embedding_dim, name=\"embedding\")(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs = outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        loss = 'binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(),'AUC','FalsePositives','TruePositives', 'FalseNegatives', 'TrueNegatives']\n",
    "    \n",
    "    )\n",
    "    \n",
    "    # print(model)\n",
    "    model.summary()\n",
    "    return model \n",
    " \n",
    " \n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "    \n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        \n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        \n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        \n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        \n",
    "        # get predictions using the transformed features\n",
    "        return model(transformed_features)\n",
    "        \n",
    "    return serve_tf_examples_fn\n",
    "    \n",
    "def run_fn(fn_args: FnArgs) -> None:\n",
    "    \n",
    "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = log_dir, update_freq='batch'\n",
    "    )\n",
    "    \n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "    \n",
    "    # Load the transform output\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "    \n",
    "    # Create batches of data\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output, 10)\n",
    "    val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)\n",
    "    vectorize_layer.adapt(\n",
    "        [j[0].numpy()[0] for j in [\n",
    "            i[0][transformed_name(FEATURE_KEY)]\n",
    "                for i in list(train_set)]])\n",
    "    \n",
    "    # Build the model\n",
    "    model = model_builder()\n",
    "    \n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(x = train_set,\n",
    "            validation_data = val_set,\n",
    "            callbacks = [tensorboard_callback, es, mc],\n",
    "            steps_per_epoch = 1000, \n",
    "            validation_steps= 1000,\n",
    "            epochs=10)\n",
    "    signatures = {\n",
    "        'serving_default':\n",
    "        _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(\n",
    "                                    tf.TensorSpec(\n",
    "                                    shape=[None],\n",
    "                                    dtype=tf.string,\n",
    "                                    name='examples'))\n",
    "    }\n",
    "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb530b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tfx.proto import trainer_pb2\n",
    " \n",
    "trainer  = Trainer(\n",
    "    module_file=os.path.abspath(TRAINER_MODULE_FILE),\n",
    "    examples = transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    train_args=trainer_pb2.TrainArgs(splits=['train']),\n",
    "    eval_args=trainer_pb2.EvalArgs(splits=['eval'])\n",
    ")\n",
    "interactive_context.run(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516077d",
   "metadata": {},
   "source": [
    "# Model Analysis And Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2aed2",
   "metadata": {},
   "source": [
    "Pada tahap ini model yang telah dibangun dan dilatih akan dianalisa dan divalidasi. Untuk melakukan hal tersebut, dibutuhkan dua buah komponen yaitu Resolver dan Evaluator. Resolver digunakan untuk membandingkan model yang lama dengan model yang baru, dan Evaluator digunakan untuk memeriksa apakah model telah bekerja sesuai threshold yang telah ditetapakan atau tidak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55877ce0",
   "metadata": {},
   "source": [
    "## Resolver Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.dsl.components.common.resolver import Resolver \n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy \n",
    "from tfx.types import Channel \n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing \n",
    " \n",
    "model_resolver = Resolver(\n",
    "    strategy_class= LatestBlessedModelStrategy,\n",
    "    model = Channel(type=Model),\n",
    "    model_blessing = Channel(type=ModelBlessing)\n",
    ").with_id('Latest_blessed_model_resolver')\n",
    " \n",
    "interactive_context.run(model_resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca8219",
   "metadata": {},
   "source": [
    "## Evaluator Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f876aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma \n",
    " \n",
    "eval_config = tfma.EvalConfig(\n",
    "    model_specs=[tfma.ModelSpec(label_key='label')],\n",
    "    slicing_specs=[tfma.SlicingSpec()],\n",
    "    metrics_specs=[\n",
    "        tfma.MetricsSpec(metrics=[\n",
    "     \n",
    "            tfma.MetricConfig(class_name='AUC'),\n",
    "            tfma.MetricConfig(class_name='FalsePositives'),\n",
    "            tfma.MetricConfig(class_name='TruePositives'),\n",
    "            tfma.MetricConfig(class_name='FalseNegatives'),\n",
    "            tfma.MetricConfig(class_name='TrueNegatives'),\n",
    "            tfma.MetricConfig(class_name='BinaryAccuracy',\n",
    "                threshold=tfma.MetricThreshold(\n",
    "                    value_threshold=tfma.GenericValueThreshold(\n",
    "                        lower_bound={'value':0.5}),\n",
    "                    change_threshold=tfma.GenericChangeThreshold(\n",
    "                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                        absolute={'value':0.0001})\n",
    "                    )\n",
    "            )\n",
    "        ])\n",
    "    ]\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import Evaluator\n",
    "evaluator = Evaluator(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    model=trainer.outputs['model'],\n",
    "    baseline_model=model_resolver.outputs['model'],\n",
    "    eval_config=eval_config)\n",
    " \n",
    "interactive_context.run(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c522dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = evaluator.outputs['evaluation'].get()[0].uri\n",
    "tfma_result = tfma.load_eval_result(eval_result)\n",
    "tfma.view.render_slicing_metrics(tfma_result)\n",
    "tfma.addons.fairness.view.widget_view.render_fairness_indicator(\n",
    "    tfma_result\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864eacdf",
   "metadata": {},
   "source": [
    "# Pusher Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1088d",
   "metadata": {},
   "source": [
    "Setelah model sesuai threshold yang telah dibuat, model dapat dideploy menggunakan komponen pusher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ddc783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tfx.components import Pusher \n",
    "from tfx.proto import pusher_pb2 \n",
    " \n",
    "pusher = Pusher(\n",
    "model=trainer.outputs['model'],\n",
    "model_blessing=evaluator.outputs['blessing'],\n",
    "push_destination=pusher_pb2.PushDestination(\n",
    "    filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "        base_directory='serving_model_dir/sms-detection-model'))\n",
    " \n",
    ")\n",
    " \n",
    "interactive_context.run(pusher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb7c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654e511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
